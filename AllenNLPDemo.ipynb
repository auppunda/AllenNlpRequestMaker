{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AllenNLPDemo.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "jP7d39uMj2My",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import requests\n",
        "import json"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-0Lx_5xqj23T",
        "colab_type": "text"
      },
      "source": [
        "#API Calls possible:\n",
        "\n",
        "For each endpoint there are a few calls possible. Each endpoint has a GET at that endpoint, a POST to /endpoint/predict, a POST to /endpoint/interpret/interpret_id, and a POST to /endpoint/attack/attack_id. The main ones that you will probably need to use are interpret and predict. In the GUI, when you click run it runs the predict endpoint on that model. Since for this server there is no ui available, we can manually just run the predict at whichever endpoint you need. \n",
        "\n",
        "The Models as of right now the server is capable of handling is named-entity-recognition, open-information-extraction, and semantic-role-labeling. If you want to run one of these models, the api call that needs to be made is at /api/MODEL_NAME/predict. \n",
        "\n",
        "For all three of the model's that the server is capable of running the corresponding json that the request needs to include in the body of the predict is of the form\n",
        "\n",
        "`\n",
        "{\n",
        "   \"sentence\": \"I like to go to the grocery store\"\n",
        "}\n",
        "`\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QsG8lYFUnQ5J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def run(api_endpoint, data):\n",
        "  url = 'http://localhost:8080' + api_endpoint\n",
        "  r = requests.post(url = url, data = json.dumps(data))\n",
        "  return r.json()"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S9PzZ6JLpeUr",
        "colab_type": "text"
      },
      "source": [
        "# Example Usage\n",
        "\n",
        "In a cell you can call the function run. Say you want to run the sentence \"However, voters decided that if the stadium was such a good idea someone would build it himself, and rejected it 59% to 41%.\" on the open information extraction model, then the user would run this code:\n",
        "\n",
        "```\n",
        "data = {\n",
        "  \"sentence\" : \"However, voters decided that if the stadium was such a good idea someone would build it himself, and rejected it 59% to 41%.\"\n",
        "}\n",
        "\n",
        "output = run('/api/open-information-extraction/predict', data)\n",
        "\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q8s0DylHq58O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data = { \"sentence\" : \"However, voters decided that if the stadium was such a good idea someone would build it himself, and rejected it 59% to 41%.\" }\n",
        "output = run('/api/semantic-role-labiling/predict', data)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}